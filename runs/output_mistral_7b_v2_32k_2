Using device: cuda
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.05s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.87s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.62s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.71s/it]
Total experiments: 56
Skipping file /home/amey/long-context-llms/prompts_new/prompts_Mistral-7B-Instruct-v0.2_32k/context_lang:de,question_lang:en,noise_type:ar,retrieval_type:semantic_similarity,needle_position:middle.csv, predictions already present
Skipping file /home/amey/long-context-llms/prompts_new/prompts_Mistral-7B-Instruct-v0.2_32k/context_lang:zh,question_lang:en,noise_type:hi,retrieval_type:semantic_similarity,needle_position:middle.csv, predictions already present
Skipping file /home/amey/long-context-llms/prompts_new/prompts_Mistral-7B-Instruct-v0.2_32k/context_lang:de,question_lang:en,noise_type:hi,retrieval_type:semantic_similarity,needle_position:middle.csv, predictions already present
Skipping file /home/amey/long-context-llms/prompts_new/prompts_Mistral-7B-Instruct-v0.2_32k/context_lang:es,question_lang:en,noise_type:vi,retrieval_type:semantic_similarity,needle_position:middle.csv, predictions already present
Skipping file /home/amey/long-context-llms/prompts_new/prompts_Mistral-7B-Instruct-v0.2_32k/context_lang:ar,question_lang:en,noise_type:zh,retrieval_type:semantic_similarity,needle_position:middle.csv, predictions already present
Skipping file /home/amey/long-context-llms/prompts_new/prompts_Mistral-7B-Instruct-v0.2_32k/context_lang:es,question_lang:en,noise_type:es,retrieval_type:semantic_similarity,needle_position:middle.csv, predictions already present
Skipping file /home/amey/long-context-llms/prompts_new/prompts_Mistral-7B-Instruct-v0.2_32k/context_lang:vi,question_lang:en,noise_type:en,retrieval_type:semantic_similarity,needle_position:middle.csv, predictions already present
Skipping file /home/amey/long-context-llms/prompts_new/prompts_Mistral-7B-Instruct-v0.2_32k/context_lang:ar,question_lang:en,noise_type:hi,retrieval_type:semantic_similarity,needle_position:middle.csv, predictions already present
Skipping file /home/amey/long-context-llms/prompts_new/prompts_Mistral-7B-Instruct-v0.2_32k/context_lang:vi,question_lang:en,noise_type:hi,retrieval_type:semantic_similarity,needle_position:middle.csv, predictions already present
Skipping file /home/amey/long-context-llms/prompts_new/prompts_Mistral-7B-Instruct-v0.2_32k/context_lang:hi,question_lang:en,noise_type:ar,retrieval_type:semantic_similarity,needle_position:middle.csv, predictions already present
Skipping file /home/amey/long-context-llms/prompts_new/prompts_Mistral-7B-Instruct-v0.2_32k/context_lang:de,question_lang:en,noise_type:es,retrieval_type:semantic_similarity,needle_position:middle.csv, predictions already present
Skipping file /home/amey/long-context-llms/prompts_new/prompts_Mistral-7B-Instruct-v0.2_32k/context_lang:es,question_lang:en,noise_type:ar,retrieval_type:semantic_similarity,needle_position:middle.csv, predictions already present
Skipping file /home/amey/long-context-llms/prompts_new/prompts_Mistral-7B-Instruct-v0.2_32k/context_lang:es,question_lang:en,noise_type:hi,retrieval_type:semantic_similarity,needle_position:middle.csv, predictions already present
Skipping file /home/amey/long-context-llms/prompts_new/prompts_Mistral-7B-Instruct-v0.2_32k/context_lang:de,question_lang:en,noise_type:multilingual,retrieval_type:semantic_similarity,needle_position:middle.csv, predictions already present
Skipping file /home/amey/long-context-llms/prompts_new/prompts_Mistral-7B-Instruct-v0.2_32k/context_lang:es,question_lang:en,noise_type:zh,retrieval_type:semantic_similarity,needle_position:middle.csv, predictions already present
Skipping file /home/amey/long-context-llms/prompts_new/prompts_Mistral-7B-Instruct-v0.2_32k/context_lang:hi,question_lang:en,noise_type:hi,retrieval_type:semantic_similarity,needle_position:middle.csv, predictions already present
Skipping file /home/amey/long-context-llms/prompts_new/prompts_Mistral-7B-Instruct-v0.2_32k/context_lang:ar,question_lang:en,noise_type:de,retrieval_type:semantic_similarity,needle_position:middle.csv, predictions already present
Skipping file /home/amey/long-context-llms/prompts_new/prompts_Mistral-7B-Instruct-v0.2_32k/context_lang:de,question_lang:en,noise_type:vi,retrieval_type:semantic_similarity,needle_position:middle.csv, predictions already present
Running inference for /home/amey/long-context-llms/prompts_new/prompts_Mistral-7B-Instruct-v0.2_32k/context_lang:vi,question_lang:en,noise_type:de,retrieval_type:semantic_similarity,needle_position:middle.csv
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
Running inference on 100 data points:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Running inference on 100 data points:   0%|          | 0/10 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/home/amey/long-context-llms/mistral-7b-instruct-v2_inference_new.py", line 149, in <module>
    predictions = inference(
  File "/home/amey/long-context-llms/mistral-7b-instruct-v2_inference_new.py", line 58, in inference
    preds = pipeline(batch_inputs)
  File "/home/amey/miniconda3/envs/socionet/lib/python3.9/site-packages/transformers/pipelines/text_generation.py", line 260, in __call__
    return super().__call__(chats, **kwargs)
  File "/home/amey/miniconda3/envs/socionet/lib/python3.9/site-packages/transformers/pipelines/base.py", line 1235, in __call__
    outputs = list(final_iterator)
  File "/home/amey/miniconda3/envs/socionet/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py", line 124, in __next__
    item = next(self.iterator)
  File "/home/amey/miniconda3/envs/socionet/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py", line 125, in __next__
    processed = self.infer(item, **self.params)
  File "/home/amey/miniconda3/envs/socionet/lib/python3.9/site-packages/transformers/pipelines/base.py", line 1161, in forward
    model_outputs = self._forward(model_inputs, **forward_params)
  File "/home/amey/miniconda3/envs/socionet/lib/python3.9/site-packages/transformers/pipelines/text_generation.py", line 349, in _forward
    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)
  File "/home/amey/miniconda3/envs/socionet/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/amey/miniconda3/envs/socionet/lib/python3.9/site-packages/transformers/generation/utils.py", line 1914, in generate
    result = self._sample(
  File "/home/amey/miniconda3/envs/socionet/lib/python3.9/site-packages/transformers/generation/utils.py", line 2651, in _sample
    outputs = self(
  File "/home/amey/miniconda3/envs/socionet/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/amey/miniconda3/envs/socionet/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/amey/miniconda3/envs/socionet/lib/python3.9/site-packages/transformers/models/mistral/modeling_mistral.py", line 1200, in forward
    outputs = self.model(
  File "/home/amey/miniconda3/envs/socionet/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/amey/miniconda3/envs/socionet/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/amey/miniconda3/envs/socionet/lib/python3.9/site-packages/transformers/models/mistral/modeling_mistral.py", line 976, in forward
    layer_outputs = decoder_layer(
  File "/home/amey/miniconda3/envs/socionet/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/amey/miniconda3/envs/socionet/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/amey/miniconda3/envs/socionet/lib/python3.9/site-packages/transformers/models/mistral/modeling_mistral.py", line 732, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/amey/miniconda3/envs/socionet/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/amey/miniconda3/envs/socionet/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/amey/miniconda3/envs/socionet/lib/python3.9/site-packages/transformers/models/mistral/modeling_mistral.py", line 171, in forward
    return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 904.00 MiB. GPU 
